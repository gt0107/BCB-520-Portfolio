ylab("NHL Games Played") +
theme(plot.title = element_text(hjust = 0.5)) +
scale_color_brewer(palette="Set1", name="Draft Round")
NHLdraftstats <- read.csv("NHLdraftstats.csv")
library(dplyr)
library(ggplot2)
players_stats_2013_2 <- NHLdraftstats %>%
filter(draftyear == 2013) %>%
filter(name != "Elias Lindholm")
ggplot(players_stats_2013_2, aes(x=postdraft, y=NHLgames, color=as.factor(round))) +
geom_smooth(se=FALSE) +
theme_minimal() +
ggtitle("Overall Players in 2013 by Draft Round") +
xlab("Post Draft Position") +
ylab("NHL Games Played") +
theme(plot.title = element_text(hjust = 0.5)) +
scale_color_brewer(palette="Set1", name="Draft Round")
library(dplyr)
library(ggplot2)
NHLdraftstats <- read.csv("NHLdraftstats.csv")
Elias_stats <- NHLdraftstats %>%
filter(name == "Elias Lindholm")
ggplot(Elias_stats, aes(x=postdraft, y=NHLgames)) +
geom_smooth() +
theme_minimal() +
ggtitle("Elias Lindholm Performance") +
xlab("Post Draft Position") +
ylab("NHL Games Played") +
theme(plot.title = element_text(hjust = 0.5))
players_stats_2013 <- NHLdraftstats %>%
filter(draftyear == 2013)
library(dplyr)
agg_data <- players_stats_2013 %>%
group_by(round, position) %>%
summarise(AvgNHLGames = mean(NHLgames, na.rm = TRUE), .groups = 'drop')
ggplot(agg_data, aes(x = factor(round), y = AvgNHLGames, fill = factor(round))) +
geom_col() +
facet_wrap(~position, scales = "free_y") +
theme_minimal() +
ggtitle("Average NHL Games Played by Draft Round and Position (2013)") +
xlab("Draft Round") +
ylab("Average NHL Games Played") +
scale_fill_brewer(palette = "Set1", name = "Draft Round") +
theme(plot.title = element_text(hjust = 0.5),
axis.text.x = element_text(angle = 45, hjust = 1, size = 12))
players_stats_2013 <- NHLdraftstats %>%
filter(draftyear == 2013)
library(dplyr)
library(ggplot2)
# Calculate the average NHL games played by players other than Elias Lindholm
average_games <- players_stats_2013 %>%
filter(name != "Elias Lindholm") %>%
summarise(average_NHL_games = mean(NHLgames, na.rm = TRUE))
# Calculate Elias Lindholm's average NHL games
elias_avg_games <- players_stats_2013 %>%
filter(name == "Elias Lindholm") %>%
summarise(NHLgames = mean(NHLgames, na.rm = TRUE)) %>%
mutate(name = "Elias Lindholm", category = "Individual")
# Create a data frame for the draft average
average_df <- data.frame(
name = "2013 Draft Average",
NHLgames = average_games$average_NHL_games,
category = "Average"
)
# Ensure both data frames have the same structure and combine them
comparison_data <- rbind(elias_avg_games, average_df)
# Create the bar graph comparing Elias Lindholm's NHL games played to the 2013 draft average
ggplot(comparison_data, aes(x = name, y = NHLgames, fill = category)) +
geom_col() +
theme_minimal() +
ggtitle("Elias Lindholm vs. 2013 Draft Average NHL Games Played") +
xlab("Category") +
ylab("NHL Games Played") +
scale_fill_manual(values = c("2013 Drafted Players" = "red", "Elias Lindholm" = "blue")) +
theme(plot.title = element_text(hjust = 0.5), axis.text.x = element_blank())
players_stats_2013 <- NHLdraftstats %>%
filter(draftyear == 2013)
library(dplyr)
library(ggplot2)
# Calculate the average NHL games played by players other than Elias Lindholm
average_games <- players_stats_2013 %>%
filter(name != "Elias Lindholm") %>%
summarise(average_NHL_games = mean(NHLgames, na.rm = TRUE))
# Calculate Elias Lindholm's average NHL games
elias_avg_games <- players_stats_2013 %>%
filter(name == "Elias Lindholm") %>%
summarise(NHLgames = mean(NHLgames, na.rm = TRUE)) %>%
mutate(name = "Elias Lindholm", category = "Individual")
# Create a data frame for the draft average
average_df <- data.frame(
name = "2013 Draft Average",
NHLgames = average_games$average_NHL_games,
category = "Average"
)
# Ensure both data frames have the same structure and combine them
comparison_data <- rbind(elias_avg_games, average_df)
# Create the bar graph comparing Elias Lindholm's NHL games played to the 2013 draft average
ggplot(comparison_data, aes(x = name, y = NHLgames, fill = category)) +
geom_col() +
theme_minimal() +
ggtitle("Elias Lindholm vs. 2013 Draft Average NHL Games Played") +
xlab("Category") +
ylab("NHL Games Played") +
scale_fill_manual(values = c("Average" = "red", "Individual" = "blue")) +
theme(plot.title = element_text(hjust = 0.5), axis.text.x = element_blank())
forwards_2013 <- players_stats_2013 %>%
filter(draftyear == 2013, position == "Forward") %>%
select(name, position, NHLgames)
library(dplyr)
# Mark Elias Lindholm and other players
players_stats_2013 <- players_stats_2013 %>%
mutate(Category = ifelse(name == "Elias Lindholm", "Elias Lindholm", "Other Players"))
# Calculate average NHL games for Elias and other players
avg_games_comparison <- players_stats_2013 %>%
group_by(Category) %>%
summarise(AverageNHLGames = mean(NHLgames, na.rm = TRUE))
# Plot
ggplot(avg_games_comparison, aes(x = Category, y = AverageNHLGames, fill = Category)) +
geom_col() +
theme_minimal() +
ggtitle("Average NHL Games: Elias Lindholm vs Other Forwards (2013 Draft)") +
xlab("") +
ylab("Average NHL Games Played") +
scale_fill_manual(values = c("Elias Lindholm" = "blue", "Other Players" = "red"))
library(dplyr)
players_stats_2018 <- NHLdraftstats %>%
filter(draftyear == 2018, position == "Forward") %>%
select(name, position, NHLgames)
library(dplyr)
library(ggplot2)
# Assuming Elias Lindholm is also a forward and you want his stats
elias_metric <- NHLdraftstats %>%
filter(name == "Elias Lindholm", position == "Forward") %>%
summarise(AverageNHLGames = mean(NHLgames, na.rm = TRUE))
# Calculate the average NHL games for forwards drafted in 2018
avg_2018_metric <- players_stats_2018 %>%
filter(position == "Forward") %>%
summarise(AverageNHLGames = mean(NHLgames, na.rm = TRUE))
# Combine the data for comparison
comparison_data <- rbind(
data.frame(Name = "Elias Lindholm", Metric = elias_metric$AverageNHLGames),
data.frame(Name = "2018 Forwards", Metric = avg_2018_metric$AverageNHLGames)
)
# Plot the comparison
ggplot(comparison_data, aes(x = Name, y = Metric, fill = Name)) +
geom_col() +
theme_minimal() +
ggtitle("Comparison: Elias Lindholm vs. 2018 Forwards") +
ylab("Average NHL Games Played") +
xlab("") +
scale_fill_manual(values = c("Elias Lindholm" = "blue", "2018 Forwards" = "red"))
library(tidyverse)
library(readxl)
library(knitr)
BSUUSDA  <- read.csv("USDABSU.csv")
knitr::kable(head(USDAUI))
library(tidyverse)
library(readxl)
library(knitr)
BSUUSDA  <- read.csv("USDABSU.csv")
knitr::kable(head(BSUUSDA))
library(tidyverse)
library(readxl)
library(knitr)
ISUUSDA  <- read.csv("USDAISU.csv")
knitr::kable(head(ISUUSDA))
library(readxl)
# Base URL for the API
base_url <- "https://www.research.gov/awardapi-service/v1/awards.json?awardeeName=%22regents+of+the+university+of+idaho%22"
printFields <- "rpp,offset,id,agency,awardeeCity,awardeeCountryCode,awardeeDistrictCode,awardeeName,awardeeStateCode,awardeeZipCode,cfdaNumber,coPDPI,date,startDate,expDate,estimatedTotalAmt,fundsObligatedAmt,ueiNumber,fundProgramName,parentUeiNumber,pdPIName,perfCity,perfCountryCode,perfDistrictCode,perfLocation,perfStateCode,perfZipCode,poName,primaryProgram,transType,title,awardee,poPhone,poEmail,awardeeAddress,perfAddress,publicationResearch,publicationConference,fundAgencyCode,awardAgencyCode,projectOutComesReport,abstractText,piFirstName,piMiddeInitial,piLastName,piEmail"
# Initialize an empty data frame to store results
NSFtoUI <- tibble()
# Number of results per page (as per API settings)
results_per_page <- 25
# Variable to keep track of the current page number
current_page <- 1
# Variable to control the loop
keep_going <- TRUE
while(keep_going) {
# Calculate the offset for the current page
offset <- (current_page - 1) * results_per_page + 1
# Construct the full URL with offset
url <- paste0(base_url, "&offset=", offset, "&printFields=", printFields)
# Make the API call
response <- GET(url)
# Check if the call was successful
if (status_code(response) == 200) {
# Extract and parse the JSON data
json_data <- content(response, type = "text", encoding = "UTF-8")
parsed_data <- fromJSON(json_data, flatten = TRUE)
# Extract the 'award' data and add to the all_awards data frame
awards_data <- parsed_data$response$award
NSFtoUI <- bind_rows(NSFtoUI, as_tibble(awards_data))
# Debug: Print the current page number and number of awards fetched
print(paste("Page:", current_page, "- Awards fetched:", length(awards_data$id)))
# Check if the current page has less than results_per_page awards, then it's the last page
if (length(awards_data$id) < results_per_page) {
keep_going <- FALSE
} else {
current_page <- current_page + 1
}
} else {
print(paste("Failed to fetch data: Status code", status_code(response)))
keep_going <- FALSE
}
}
install.packages("httr")
install.packages("httr2")
install.packages("jsonlite")
library(readxl)
library(httr)
library(jsonlite)
library(dplyr)
# Base URL for the API
base_url <- "https://www.research.gov/awardapi-service/v1/awards.json?awardeeName=%22regents+of+the+university+of+idaho%22"
printFields <- "rpp,offset,id,agency,awardeeCity,awardeeCountryCode,awardeeDistrictCode,awardeeName,awardeeStateCode,awardeeZipCode,cfdaNumber,coPDPI,date,startDate,expDate,estimatedTotalAmt,fundsObligatedAmt,ueiNumber,fundProgramName,parentUeiNumber,pdPIName,perfCity,perfCountryCode,perfDistrictCode,perfLocation,perfStateCode,perfZipCode,poName,primaryProgram,transType,title,awardee,poPhone,poEmail,awardeeAddress,perfAddress,publicationResearch,publicationConference,fundAgencyCode,awardAgencyCode,projectOutComesReport,abstractText,piFirstName,piMiddeInitial,piLastName,piEmail"
# Initialize an empty data frame to store results
NSFtoUI <- tibble()
# Number of results per page (as per API settings)
results_per_page <- 25
# Variable to keep track of the current page number
current_page <- 1
# Variable to control the loop
keep_going <- TRUE
while(keep_going) {
# Calculate the offset for the current page
offset <- (current_page - 1) * results_per_page + 1
# Construct the full URL with offset
url <- paste0(base_url, "&offset=", offset, "&printFields=", printFields)
# Make the API call
response <- GET(url)
# Check if the call was successful
if (status_code(response) == 200) {
# Extract and parse the JSON data
json_data <- content(response, type = "text", encoding = "UTF-8")
parsed_data <- fromJSON(json_data, flatten = TRUE)
# Extract the 'award' data and add to the all_awards data frame
awards_data <- parsed_data$response$award
NSFtoUI <- bind_rows(NSFtoUI, as_tibble(awards_data))
# Debug: Print the current page number and number of awards fetched
print(paste("Page:", current_page, "- Awards fetched:", length(awards_data$id)))
# Check if the current page has less than results_per_page awards, then it's the last page
if (length(awards_data$id) < results_per_page) {
keep_going <- FALSE
} else {
current_page <- current_page + 1
}
} else {
print(paste("Failed to fetch data: Status code", status_code(response)))
keep_going <- FALSE
}
}
View(NSFtoUI)
library(readxl)
library(httr)
library(jsonlite)
library(dplyr)
# Base URL for the API
base_url <- "https://www.research.gov/awardapi-service/v1/awards.json?awardeeName=%22boise+state+university%22"
printFields <- "rpp,offset,id,agency,awardeeCity,awardeeCountryCode,awardeeDistrictCode,awardeeName,awardeeStateCode,awardeeZipCode,cfdaNumber,coPDPI,date,startDate,expDate,estimatedTotalAmt,fundsObligatedAmt,ueiNumber,fundProgramName,parentUeiNumber,pdPIName,perfCity,perfCountryCode,perfDistrictCode,perfLocation,perfStateCode,perfZipCode,poName,primaryProgram,transType,title,awardee,poPhone,poEmail,awardeeAddress,perfAddress,publicationResearch,publicationConference,fundAgencyCode,awardAgencyCode,projectOutComesReport,abstractText,piFirstName,piMiddeInitial,piLastName,piEmail"
# Initialize an empty data frame to store results
NSFtoUI <- tibble()
# Number of results per page (as per API settings)
results_per_page <- 25
# Variable to keep track of the current page number
current_page <- 1
# Variable to control the loop
keep_going <- TRUE
while(keep_going) {
# Calculate the offset for the current page
offset <- (current_page - 1) * results_per_page + 1
# Construct the full URL with offset
url <- paste0(base_url, "&offset=", offset, "&printFields=", printFields)
# Make the API call
response <- GET(url)
# Check if the call was successful
if (status_code(response) == 200) {
# Extract and parse the JSON data
json_data <- content(response, type = "text", encoding = "UTF-8")
parsed_data <- fromJSON(json_data, flatten = TRUE)
# Extract the 'award' data and add to the all_awards data frame
awards_data <- parsed_data$response$award
NSFBSU <- bind_rows(NSFBSU, as_tibble(awards_data))
# Debug: Print the current page number and number of awards fetched
print(paste("Page:", current_page, "- Awards fetched:", length(awards_data$id)))
# Check if the current page has less than results_per_page awards, then it's the last page
if (length(awards_data$id) < results_per_page) {
keep_going <- FALSE
} else {
current_page <- current_page + 1
}
} else {
print(paste("Failed to fetch data: Status code", status_code(response)))
keep_going <- FALSE
}
}
library(readxl)
library(httr)
library(jsonlite)
library(dplyr)
# Base URL for the API
base_url <- "https://www.research.gov/awardapi-service/v1/awards.json?awardeeName=%22boise+state+university%22"
printFields <- "rpp,offset,id,agency,awardeeCity,awardeeCountryCode,awardeeDistrictCode,awardeeName,awardeeStateCode,awardeeZipCode,cfdaNumber,coPDPI,date,startDate,expDate,estimatedTotalAmt,fundsObligatedAmt,ueiNumber,fundProgramName,parentUeiNumber,pdPIName,perfCity,perfCountryCode,perfDistrictCode,perfLocation,perfStateCode,perfZipCode,poName,primaryProgram,transType,title,awardee,poPhone,poEmail,awardeeAddress,perfAddress,publicationResearch,publicationConference,fundAgencyCode,awardAgencyCode,projectOutComesReport,abstractText,piFirstName,piMiddeInitial,piLastName,piEmail"
# Initialize an empty data frame to store results
NSFBSU <- tibble()
# Number of results per page (as per API settings)
results_per_page <- 25
# Variable to keep track of the current page number
current_page <- 1
# Variable to control the loop
keep_going <- TRUE
while(keep_going) {
# Calculate the offset for the current page
offset <- (current_page - 1) * results_per_page + 1
# Construct the full URL with offset
url <- paste0(base_url, "&offset=", offset, "&printFields=", printFields)
# Make the API call
response <- GET(url)
# Check if the call was successful
if (status_code(response) == 200) {
# Extract and parse the JSON data
json_data <- content(response, type = "text", encoding = "UTF-8")
parsed_data <- fromJSON(json_data, flatten = TRUE)
# Extract the 'award' data and add to the all_awards data frame
awards_data <- parsed_data$response$award
NSFBSU <- bind_rows(NSFBSU, as_tibble(awards_data))
# Debug: Print the current page number and number of awards fetched
print(paste("Page:", current_page, "- Awards fetched:", length(awards_data$id)))
# Check if the current page has less than results_per_page awards, then it's the last page
if (length(awards_data$id) < results_per_page) {
keep_going <- FALSE
} else {
current_page <- current_page + 1
}
} else {
print(paste("Failed to fetch data: Status code", status_code(response)))
keep_going <- FALSE
}
}
library(readxl)
library(httr)
library(jsonlite)
library(dplyr)
# Base URL for the API
base_url <- "https://www.research.gov/awardapi-service/v1/awards.json?awardeeName=%22boise+state+university%22"
printFields <- "rpp,offset,id,agency,awardeeCity,awardeeCountryCode,awardeeDistrictCode,awardeeName,awardeeStateCode,awardeeZipCode,cfdaNumber,coPDPI,date,startDate,expDate,estimatedTotalAmt,fundsObligatedAmt,ueiNumber,fundProgramName,parentUeiNumber,pdPIName,perfCity,perfCountryCode,perfDistrictCode,perfLocation,perfStateCode,perfZipCode,poName,primaryProgram,transType,title,awardee,poPhone,poEmail,awardeeAddress,perfAddress,publicationResearch,publicationConference,fundAgencyCode,awardAgencyCode,projectOutComesReport,abstractText,piFirstName,piMiddeInitial,piLastName,piEmail"
# Initialize an empty data frame to store results
NSFtoUI <- tibble()
# Number of results per page (as per API settings)
results_per_page <- 25
# Variable to keep track of the current page number
current_page <- 1
# Variable to control the loop
keep_going <- TRUE
while(keep_going) {
# Calculate the offset for the current page
offset <- (current_page - 1) * results_per_page + 1
# Construct the full URL with offset
url <- paste0(base_url, "&offset=", offset, "&printFields=", printFields)
# Make the API call
response <- GET(url)
# Check if the call was successful
if (status_code(response) == 200) {
# Extract and parse the JSON data
json_data <- content(response, type = "text", encoding = "UTF-8")
parsed_data <- fromJSON(json_data, flatten = TRUE)
# Extract the 'award' data and add to the all_awards data frame
awards_data <- parsed_data$response$award
NSFBSU <- bind_rows(NSFBSU, as_tibble(awards_data))
# Debug: Print the current page number and number of awards fetched
print(paste("Page:", current_page, "- Awards fetched:", length(awards_data$id)))
# Check if the current page has less than results_per_page awards, then it's the last page
if (length(awards_data$id) < results_per_page) {
keep_going <- FALSE
} else {
current_page <- current_page + 1
}
} else {
print(paste("Failed to fetch data: Status code", status_code(response)))
keep_going <- FALSE
}
}
library(readxl)
library(httr)
library(jsonlite)
library(dplyr)
# Base URL for the API
base_url <- "https://www.research.gov/awardapi-service/v1/awards.json?awardeeName=%22boise+state+university%22"
printFields <- "rpp,offset,id,agency,awardeeCity,awardeeCountryCode,awardeeDistrictCode,awardeeName,awardeeStateCode,awardeeZipCode,cfdaNumber,coPDPI,date,startDate,expDate,estimatedTotalAmt,fundsObligatedAmt,ueiNumber,fundProgramName,parentUeiNumber,pdPIName,perfCity,perfCountryCode,perfDistrictCode,perfLocation,perfStateCode,perfZipCode,poName,primaryProgram,transType,title,awardee,poPhone,poEmail,awardeeAddress,perfAddress,publicationResearch,publicationConference,fundAgencyCode,awardAgencyCode,projectOutComesReport,abstractText,piFirstName,piMiddeInitial,piLastName,piEmail"
# Initialize an empty data frame to store results
NSFtoUI <- tibble()
# Number of results per page (as per API settings)
results_per_page <- 25
# Variable to keep track of the current page number
current_page <- 1
# Variable to control the loop
keep_going <- TRUE
while(keep_going) {
# Calculate the offset for the current page
offset <- (current_page - 1) * results_per_page + 1
# Construct the full URL with offset
url <- paste0(base_url, "&offset=", offset, "&printFields=", printFields)
# Make the API call
response <- GET(url)
# Check if the call was successful
if (status_code(response) == 200) {
# Extract and parse the JSON data
json_data <- content(response, type = "text", encoding = "UTF-8")
parsed_data <- fromJSON(json_data, flatten = TRUE)
# Extract the 'award' data and add to the all_awards data frame
awards_data <- parsed_data$response$award
NSFBSU <- bind_rows(NSFBSU, as_tibble(awards_data))
# Debug: Print the current page number and number of awards fetched
print(paste("Page:", current_page, "- Awards fetched:", length(awards_data$id)))
# Check if the current page has less than results_per_page awards, then it's the last page
if (length(awards_data$id) < results_per_page) {
keep_going <- FALSE
} else {
current_page <- current_page + 1
}
} else {
print(paste("Failed to fetch data: Status code", status_code(response)))
keep_going <- FALSE
}
}
library(readxl)
library(httr)
library(jsonlite)
library(dplyr)
# Base URL for the API
base_url <- "https://www.research.gov/awardapi-service/v1/awards.json?awardeeName=%22boise+state+university%22"
printFields <- "rpp,offset,id,agency,awardeeCity,awardeeCountryCode,awardeeDistrictCode,awardeeName,awardeeStateCode,awardeeZipCode,cfdaNumber,coPDPI,date,startDate,expDate,estimatedTotalAmt,fundsObligatedAmt,ueiNumber,fundProgramName,parentUeiNumber,pdPIName,perfCity,perfCountryCode,perfDistrictCode,perfLocation,perfStateCode,perfZipCode,poName,primaryProgram,transType,title,awardee,poPhone,poEmail,awardeeAddress,perfAddress,publicationResearch,publicationConference,fundAgencyCode,awardAgencyCode,projectOutComesReport,abstractText,piFirstName,piMiddeInitial,piLastName,piEmail"
# Initialize an empty data frame to store results
NSFBSU <- tibble()
# Number of results per page (as per API settings)
results_per_page <- 25
# Variable to keep track of the current page number
current_page <- 1
# Variable to control the loop
keep_going <- TRUE
while(keep_going) {
# Calculate the offset for the current page
offset <- (current_page - 1) * results_per_page + 1
# Construct the full URL with offset
url <- paste0(base_url, "&offset=", offset, "&printFields=", printFields)
# Make the API call
response <- GET(url)
# Check if the call was successful
if (status_code(response) == 200) {
# Extract and parse the JSON data
json_data <- content(response, type = "text", encoding = "UTF-8")
parsed_data <- fromJSON(json_data, flatten = TRUE)
# Extract the 'award' data and add to the all_awards data frame
awards_data <- parsed_data$response$award
NSFBSU <- bind_rows(NSFBSU, as_tibble(awards_data))
# Debug: Print the current page number and number of awards fetched
print(paste("Page:", current_page, "- Awards fetched:", length(awards_data$id)))
# Check if the current page has less than results_per_page awards, then it's the last page
if (length(awards_data$id) < results_per_page) {
keep_going <- FALSE
} else {
current_page <- current_page + 1
}
} else {
print(paste("Failed to fetch data: Status code", status_code(response)))
keep_going <- FALSE
}
}
View(NSFBSU)
library(readxl)
library(httr)
library(jsonlite)
library(dplyr)
# Base URL for the API
base_url <- "https://www.research.gov/awardapi-service/v1/awards.json?awardeeName=%22boise+state+university%22"
printFields <- "rpp,offset,id,agency,awardeeCity,awardeeCountryCode,awardeeDistrictCode,awardeeName,awardeeStateCode,awardeeZipCode,cfdaNumber,coPDPI,date,startDate,expDate,estimatedTotalAmt,fundsObligatedAmt,ueiNumber,fundProgramName,parentUeiNumber,pdPIName,perfCity,perfCountryCode,perfDistrictCode,perfLocation,perfStateCode,perfZipCode,poName,primaryProgram,transType,title,awardee,poPhone,poEmail,awardeeAddress,perfAddress,publicationResearch,publicationConference,fundAgencyCode,awardAgencyCode,projectOutComesReport,abstractText,piFirstName,piMiddeInitial,piLastName,piEmail"
# Initialize an empty data frame to store results
NSFtoUI <- tibble()
# Number of results per page (as per API settings)
results_per_page <- 25
# Variable to keep track of the current page number
current_page <- 1
# Variable to control the loop
keep_going <- TRUE
while(keep_going) {
# Calculate the offset for the current page
offset <- (current_page - 1) * results_per_page + 1
# Construct the full URL with offset
url <- paste0(base_url, "&offset=", offset, "&printFields=", printFields)
# Make the API call
response <- GET(url)
# Check if the call was successful
if (status_code(response) == 200) {
# Extract and parse the JSON data
json_data <- content(response, type = "text", encoding = "UTF-8")
parsed_data <- fromJSON(json_data, flatten = TRUE)
# Extract the 'award' data and add to the all_awards data frame
awards_data <- parsed_data$response$award
NSFBSU <- bind_rows(NSFBSU, as_tibble(awards_data))
# Debug: Print the current page number and number of awards fetched
print(paste("Page:", current_page, "- Awards fetched:", length(awards_data$id)))
# Check if the current page has less than results_per_page awards, then it's the last page
if (length(awards_data$id) < results_per_page) {
keep_going <- FALSE
} else {
current_page <- current_page + 1
}
} else {
print(paste("Failed to fetch data: Status code", status_code(response)))
keep_going <- FALSE
}
}
